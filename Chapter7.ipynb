{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning and Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example of an Ensemble method, you can train a group of Decision Tree classifiers, each on a different random subset of the training set. To make predictions, you obtain the predictions of all the individual trees, then predict the class that gets the most votes. Such an ensemble of Decision Tree is called a *Random Forest*, and despite its simplicity, this is one of the most powerful Machine Learning algorithms available today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very simple way to create an even better classifier is to aggregate the predictions of each classifier and predict the class that gets the most votes. This majority-vote classifier is called a *hard voting* classifier.\n",
    "\n",
    "*Law of large numbers*: as you keep tossing the coin, the ratio of heads gets closer and closer to the probability of heads (51% vs 49%).\n",
    "\n",
    "Ensemble methods work best when the predictors are as independent from one another as possible. One way to get diverse classifiers is to train them using very difficult algorithms. This increases the chance that they will make very different types of errors, improving the ensemble's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr',\n",
       "                              LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                 dual=False, fit_intercept=True,\n",
       "                                                 intercept_scaling=1,\n",
       "                                                 l1_ratio=None, max_iter=100,\n",
       "                                                 multi_class='auto',\n",
       "                                                 n_jobs=None, penalty='l2',\n",
       "                                                 random_state=None,\n",
       "                                                 solver='lbfgs', tol=0.0001,\n",
       "                                                 verbose=0, warm_start=False)),\n",
       "                             ('rf',\n",
       "                              RandomForestClassifier(bootstrap=True,\n",
       "                                                     ccp_alpha=0.0,\n",
       "                                                     class_weight=None,\n",
       "                                                     cr...\n",
       "                                                     oob_score=False,\n",
       "                                                     random_state=None,\n",
       "                                                     verbose=0,\n",
       "                                                     warm_start=False)),\n",
       "                             ('svc',\n",
       "                              SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                                  class_weight=None, coef0=0.0,\n",
       "                                  decision_function_shape='ovr', degree=3,\n",
       "                                  gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                                  probability=False, random_state=None,\n",
       "                                  shrinking=True, tol=0.001, verbose=False))],\n",
       "                 flatten_transform=True, n_jobs=None, voting='hard',\n",
       "                 weights=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.888\n",
      "SVC 0.896\n",
      "VotingClassifier 0.904\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__,accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all classifiers are able to estimate class probabilities (i.e., they all have a predict_proba() method), then you can tell Scikit-Learn to predict the class with the highest class probability, averaged over all the individual classifiers. This is called *soft voting*. It often achieves higher performance than hard voting because it gives more weight to highly confident votes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr',\n",
       "                              LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                 dual=False, fit_intercept=True,\n",
       "                                                 intercept_scaling=1,\n",
       "                                                 l1_ratio=None, max_iter=100,\n",
       "                                                 multi_class='auto',\n",
       "                                                 n_jobs=None, penalty='l2',\n",
       "                                                 random_state=42,\n",
       "                                                 solver='lbfgs', tol=0.0001,\n",
       "                                                 verbose=0, warm_start=False)),\n",
       "                             ('rf',\n",
       "                              RandomForestClassifier(bootstrap=True,\n",
       "                                                     ccp_alpha=0.0,\n",
       "                                                     class_weight=None,\n",
       "                                                     crit...\n",
       "                                                     oob_score=False,\n",
       "                                                     random_state=42, verbose=0,\n",
       "                                                     warm_start=False)),\n",
       "                             ('svc',\n",
       "                              SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                                  class_weight=None, coef0=0.0,\n",
       "                                  decision_function_shape='ovr', degree=3,\n",
       "                                  gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                                  probability=True, random_state=42,\n",
       "                                  shrinking=True, tol=0.001, verbose=False))],\n",
       "                 flatten_transform=True, n_jobs=None, voting='soft',\n",
       "                 weights=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm_clf = SVC(gamma=\"scale\", probability=True, random_state=42)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='soft')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.896\n",
      "SVC 0.896\n",
      "VotingClassifier 0.92\n"
     ]
    }
   ],
   "source": [
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging and Pasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to get a diverse set of classifiers is to use very different training algorithms, as just discussed. Another approach is to use the same training algorithmfor every predictor and train them on different random subsets of the training set. When sampling is performed with replacement, this is called *bagging*. When sampling is performed without replacement, it is called pasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging and Pasting in Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When sampling is performed with replacement, this method is called *bagging*. When sampling is performed without replacement, it is called *pasting*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500,\n",
    "    max_samples=100, bootstrap=True, n_jobs=-1)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.912\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.856\n"
     ]
    }
   ],
   "source": [
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "y_pred_tree = tree_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BaggingClassifier automatically performs soft voting instead of hard voting if the base classifier can estimate class probabilities, which is the case with Decision Tree classifiers.\n",
    "\n",
    "Bootstrapping introduces a bit more diversity in the subsets that each predictor is trained on, so bagging ends up with a slightly higher bas than pasting: but the extra diversity also means that the predictors end up being less correlated, so the ensemble's variance is reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-Bag Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only about 63% of the training set instances are samples on average for each predictor. The remaining 37% of the training instances that are not sampled are called out-of-bag (oob) instances. Note that they are not the same 37% for all predictors.\n",
    "\n",
    "Since a predictor never sees the oob instance during training, it can be evaluated on these instances, without the need for a separate validation set. You can evaluate the ensemble itself by averaging out the oob evaluations of each predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8986666666666666"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500, \n",
    "    bootstrap=True, n_jobs=-1, oob_score=True)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.904"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Patches and Random Subspaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BaggingClassifier class supports sampling the features as well. Sampling is controlled by two hyperparameters: max_features and bootstrap_features. They work the same way as max_samples and bootstrap, but for feature sampling instead of instance sampling. Thus, each predictor will be trained on a random subset of the input features.\n",
    "\n",
    "This technique is particularly useful when you are dealing with high-dimensional inputs. Sampling both training and features is called the *Random Patches* method. Keeping all training instances (by setting boostrap=False and max_samples=1.0), but sampling features (by setting boostrap_features to True and/or max_features to a value smaller than 1.0) is called the *Random Subspace* method.\n",
    "\n",
    "(https://homl.info/23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest algorithm introduces extra randomness when growing trees; instead of searching for the very best feature when splitting node, it searches for the best feature among a random subset of features. The algorithm results in a greater tree diversity, which trades a higher bias for a lower variance, generally yielding an overall better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Similar rnd_clf\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16, random_state=42),\n",
    "    n_estimators=500, max_samples=1.0, bootstrap=True, random_state=42)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.992"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.sum(y_pred == y_pred_rf) / len(y_pred)  # almost identical predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra-Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to make trees even more random by also using random thresholds for each feature rather than searching for the best possible threshold.\n",
    "\n",
    "A forest of such extremely random is called trees is called *Extremely Randomized Trees* ensemble. Once again this technique tardes more bias for a lower variance, because finding the best possible threshold for each feature at every node is one of the most time-consuming tasks of growing the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
       "                     criterion='gini', max_depth=None, max_features='auto',\n",
       "                     max_leaf_nodes=16, max_samples=None,\n",
       "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                     min_samples_leaf=1, min_samples_split=2,\n",
       "                     min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n",
       "                     oob_score=False, random_state=None, verbose=0,\n",
       "                     warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "ext_clf = ExtraTreesClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "ext_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet another great quality of Random Forests is that they make it easy to measure the relative importance of each feature. Scikit-Learn measures a feature's importance by looking how much the tree nodes that use that feature reduce impurity on average (across all trees in the forest). More precisely, it is weighted average, where each node's weight is equal to the number of training samples that are associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.09158986704689198\n",
      "sepal width (cm) 0.026253326962093818\n",
      "petal length (cm) 0.44018052874593844\n",
      "petal width (cm) 0.44197627724507577\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
    "\n",
    "for name,score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests are very handy to get a quick understanding of what features actually matters, in particular if you need to perform feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Boosting* (originally called *hypothesis boosting*) refers to any Ensemble method that can combine several weak learners into a strong learner. The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor. There are many boosting methods available, but by far the most popular are *AdaBoost* and *Gradient Boosting*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way for a new predictor to correct its predecessor is to pay a bit more attention to the training instances that the predecessor underfitted. This is the result in new predictors focusing more and more on the hard classes. This is the technique used by AdaBoost.\n",
    "\n",
    "SVMs are generally not good base predictors for AdaBoost; they are slow and tend to be unstable with it.\n",
    "\n",
    "This sequential learning technique it cannot be parallelized, since each predictor can only be trained after the previous predictor has been trained and evaluated. As a result, it does not scale as well as bagging or pasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_decision_boundary(clf, X, y, axes=[-1.5, 2.45, -1, 1.5], alpha=0.5, contour=True):\n",
    "    x1s = np.linspace(axes[0], axes[1], 100)\n",
    "    x2s = np.linspace(axes[2], axes[3], 100)\n",
    "    x1, x2 = np.meshgrid(x1s, x2s)\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
    "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n",
    "    if contour:\n",
    "        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n",
    "        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n",
    "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", alpha=alpha)\n",
    "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", alpha=alpha)\n",
    "    plt.axis(axes)\n",
    "    plt.xlabel(r\"$x_1$\", fontsize=18)\n",
    "    plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "                   base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n",
       "                                                         class_weight=None,\n",
       "                                                         criterion='gini',\n",
       "                                                         max_depth=1,\n",
       "                                                         max_features=None,\n",
       "                                                         max_leaf_nodes=None,\n",
       "                                                         min_impurity_decrease=0.0,\n",
       "                                                         min_impurity_split=None,\n",
       "                                                         min_samples_leaf=1,\n",
       "                                                         min_samples_split=2,\n",
       "                                                         min_weight_fraction_leaf=0.0,\n",
       "                                                         presort='deprecated',\n",
       "                                                         random_state=None,\n",
       "                                                         splitter='best'),\n",
       "                   learning_rate=0.5, n_estimators=200, random_state=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "    algorithm=\"SAMME.R\",learning_rate=0.5)\n",
    "\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_decision_boundary(ada_clf, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your AdaBoost ensemble is overfitting the training set, you can try reducing the number of estimators or more strongly regularizing the base estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like AdaBoost, Gradient Boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of tweaking the instance weights at every iteration like AdaBoost does, this method tries to fit the new predictor to the *residual errors* made by the previous predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=2,\n",
       "                      max_features=None, max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                      random_state=42, splitter='best')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=2,\n",
       "                      max_features=None, max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                      random_state=42, splitter='best')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2 = y - tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg2.fit(X, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=2,\n",
       "                      max_features=None, max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                      random_state=42, splitter='best')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg3.fit(X, y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75026781])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = np.array([[0.8]])\n",
    "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3)) #Ensemble C 3 trees\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n",
       "                          init=None, learning_rate=1.0, loss='ls', max_depth=2,\n",
       "                          max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=3,\n",
       "                          n_iter_no_change=None, presort='deprecated',\n",
       "                          random_state=None, subsample=1.0, tol=0.0001,\n",
       "                          validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
    "gbrt.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning_rate hyperparameter scales the contribution of each tree. If you set it to a low value such as 0.1, you will need more trees in the ensemble to fit the training set, but the predictions will usually generalize better. This is a regularization technique called *shrinkage*.\n",
    "\n",
    "In order to find the optimal number of trees, you can use early stopping. A simple way to implement this is to use the staged_predict() method: it returns an iterator over the predictions made by the enseble at each stage of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n",
       "                          init=None, learning_rate=0.1, loss='ls', max_depth=2,\n",
       "                          max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=53,\n",
       "                          n_iter_no_change=None, presort='deprecated',\n",
       "                          random_state=None, subsample=1.0, tol=0.0001,\n",
       "                          validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "errors = [mean_squared_error(y_val, y_pred) for y_pred in gbrt.staged_predict(X_val)]\n",
    "bst_n_estimators = np.argmin(errors) + 1\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators)\n",
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to implement early stopping by actually stopping training early. You can do so by setting warm_start = True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)\n",
    "\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "for n_estimators in range(1, 120):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    y_pred = gbrt.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val, y_pred)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == 5:\n",
    "            break  # early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    }
   ],
   "source": [
    "print(gbrt.n_estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GradientBoostingRegressor class also supports a subsample hyperparameter, which specifies the fraction of training instances to be used for training each tree. This technique trades a higher bias for a lower varianve. It also speeds up training considerably. This is called *Stochastic Gradient Boosting*.\n",
    "\n",
    "It is worth to note that an optimized implementation of Gradient Boosting is available in the popular Python library XGBoost, which stands for Extreme Gradient Boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "xgb_reg = xgboost.XGBRegressor(objective ='reg:squarederror') #avoid the msg deprecated\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "y_pred = xgb_reg.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:0.264628\n",
      "Will train until validation_0-rmse hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-rmse:0.238158\n",
      "[2]\tvalidation_0-rmse:0.213671\n",
      "[3]\tvalidation_0-rmse:0.191839\n",
      "[4]\tvalidation_0-rmse:0.173536\n",
      "[5]\tvalidation_0-rmse:0.15676\n",
      "[6]\tvalidation_0-rmse:0.142701\n",
      "[7]\tvalidation_0-rmse:0.130993\n",
      "[8]\tvalidation_0-rmse:0.119751\n",
      "[9]\tvalidation_0-rmse:0.11102\n",
      "[10]\tvalidation_0-rmse:0.103301\n",
      "[11]\tvalidation_0-rmse:0.096759\n",
      "[12]\tvalidation_0-rmse:0.090696\n",
      "[13]\tvalidation_0-rmse:0.085071\n",
      "[14]\tvalidation_0-rmse:0.080967\n",
      "[15]\tvalidation_0-rmse:0.076923\n",
      "[16]\tvalidation_0-rmse:0.073651\n",
      "[17]\tvalidation_0-rmse:0.070516\n",
      "[18]\tvalidation_0-rmse:0.067908\n",
      "[19]\tvalidation_0-rmse:0.065979\n",
      "[20]\tvalidation_0-rmse:0.064584\n",
      "[21]\tvalidation_0-rmse:0.063215\n",
      "[22]\tvalidation_0-rmse:0.062086\n",
      "[23]\tvalidation_0-rmse:0.061187\n",
      "[24]\tvalidation_0-rmse:0.060842\n",
      "[25]\tvalidation_0-rmse:0.060447\n",
      "[26]\tvalidation_0-rmse:0.060134\n",
      "[27]\tvalidation_0-rmse:0.059679\n",
      "[28]\tvalidation_0-rmse:0.059234\n",
      "[29]\tvalidation_0-rmse:0.058956\n",
      "[30]\tvalidation_0-rmse:0.059107\n",
      "[31]\tvalidation_0-rmse:0.059167\n",
      "Stopping. Best iteration:\n",
      "[29]\tvalidation_0-rmse:0.058956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_reg.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=2)\n",
    "y_pred = xgb_reg.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Stacking* is based on a simple idea: instead of using trivial functions (such as hard voting) to aggregate the predictions of all predictors in an ensemble, why don't we train a model to perform this aggregation? (i.e. [3.1, 2.7, 2.9] = 3.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
