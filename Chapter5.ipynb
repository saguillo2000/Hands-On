{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *Support Vector Machine* (SVM) is a powerful and versatile Machine Learning model, capable of performing linear or nonlinear classification, regression, and even outlier detection. SVMs are particularly well suited for classification of complex small- or medium-sized datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 5.1 good explanaiton\n",
    "\n",
    "You can thjink of an SVM classifier as fitting the widest possible street (represented by the parallel dashed lines) between the class. This is called *larged margin classification*.\n",
    "\n",
    "Notice that adding more training instances \"off the street\" will not affect the decision boundary at all: it is fully determined (or \"supported\") by the instances located on the edge of the street. These instances are called the *support vectors*.\n",
    "\n",
    "SVMs are sensitive to the feature scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Margin Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we strictly impose all instances must be off the street and on the right side, this is called *hard margin classification*.The two main issued are:\n",
    "- It only work if the data is linearly separable.\n",
    "- It is sensitive to outliers\n",
    "\n",
    "The objective is to find a good balance between keeping, the street as large as possible and limiting the *margin violations* (i.e., instances that ends up in the middle of the street or even on the wrong side). This is called *soft margin classification*.\n",
    "\n",
    "If your SVM model is overfitting, you can try regularizing it by reducing C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2,3)]  #petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64) #Iris virginica\n",
    "\n",
    "svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")),\n",
    "])\n",
    "\n",
    "svm_clf.fit(X, y)\n",
    "\n",
    "svm_clf.predict([[5.5, 1.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike Logistic Reegression classifiers, SVM classifiers do not output probabilities for each class.\n",
    "\n",
    "Instead of using the LinearSVC class, we could use the SVC class with a linear kernel (SVC(kernel=\"linear\", C=1)).\n",
    "\n",
    "For better performance ypus hould set the dual hyperparameter to False, unless there are more features than training instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear SVM Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One approach to handling nonlinear datasetsis to add more features, such as polynomial features; in some cases this can result in a linearly separable dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('poly_features',\n",
       "                 PolynomialFeatures(degree=3, include_bias=True,\n",
       "                                    interaction_only=False, order='C')),\n",
       "                ('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('svm_clf',\n",
       "                 LinearSVC(C=10, class_weight=None, dual=True,\n",
       "                           fit_intercept=True, intercept_scaling=1,\n",
       "                           loss='hinge', max_iter=10000, multi_class='ovr',\n",
       "                           penalty='l2', random_state=None, tol=0.0001,\n",
       "                           verbose=0))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.15)\n",
    "polynomial_svm_clf = Pipeline([\n",
    "    (\"poly_features\", PolynomialFeatures(degree=3)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\",max_iter=10000)) #Increase the iterations\n",
    "])\n",
    "\n",
    "polynomial_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding polynomial features is simple to implement and can work great with all sorts of Machine Learning algorithms. \n",
    "\n",
    "Fortunately, when using SVMs you can apply an almost miraculous mathematical technique called the *kernel trick*. The kernel trick makes it possible to get the same result as if you added many polynomial features, even with very high-degree polynomials, wthout having actually to add them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('svm_clf',\n",
       "                 SVC(C=5, break_ties=False, cache_size=200, class_weight=None,\n",
       "                     coef0=1, decision_function_shape='ovr', degree=3,\n",
       "                     gamma='scale', kernel='poly', max_iter=-1,\n",
       "                     probability=False, random_state=None, shrinking=True,\n",
       "                     tol=0.001, verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "poly_kernel_svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
    "])\n",
    "\n",
    "poly_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another technique to tackle nonlinear problems is to add features computed using a *similarity function*, which measures how much each instance a particular *landmark*.\n",
    "\n",
    "**Equation Gaussian RBF similarity function**\n",
    "\n",
    "$\\phi_{y}(x,l) = exp (-\\gamma \\lVert x - l \\rVert^{2})$\n",
    "\n",
    "This is a bell-shaped function varying from 0 (very far away from the landmark) to 1 (at the landmark).\n",
    "\n",
    "The simplest approach on select the landmarks is create one at the location of each and every instance in the dataset. Doing that creates many dimensions and thus increases the chances that the transformed training set will be linearly eparable. The downside is that the number of features is going to be the same as number of instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian RBF Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('svm_clf',\n",
       "                 SVC(C=0.001, break_ties=False, cache_size=200,\n",
       "                     class_weight=None, coef0=0.0,\n",
       "                     decision_function_shape='ovr', degree=3, gamma=5,\n",
       "                     kernel='rbf', max_iter=-1, probability=False,\n",
       "                     random_state=None, shrinking=True, tol=0.001,\n",
       "                     verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbf_kernel_svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n",
    "])\n",
    "\n",
    "rbf_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other kernels exist but are used more rarely. Some kernels are specialized for specific data structures. *String kernels* are sometimes used when classifying text documents or DNA sequences.\n",
    "\n",
    "As a rule of thumb, you should always try the linear kernel first, especially if the training set is very large or if it has plenty of features. If the training set is not too large, ypu should also try the Gaussian RBF kernel; it works well in most cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM algorithm is versatile: not only does it support linear and nonlinear classification, but it also supports linear and nonlinear regression.\n",
    "\n",
    "the objective is the opposite one; try to fit as many instances as possible one the street while limiting margin violations. The width of the street is controlled by a hyperparameter, $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=1.0, dual=True, epsilon=1.5, fit_intercept=True,\n",
       "          intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,\n",
       "          random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "svm_reg = LinearSVR(epsilon=1.5)\n",
    "svm_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tackle nonlinear regression tasks, you can use a kernelized SVM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=100, cache_size=200, coef0=0.0, degree=2, epsilon=0.1, gamma='scale',\n",
       "    kernel='poly', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\n",
    "svm_poly_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVR class is the regression equivalent of the SVC class, and the LinearSVR class is the regression equivalent of the LinearSVC class.\n",
    "\n",
    "SVMs can also be used for outlier detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Under the Hood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Function and Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear SVM classifier model predicts the class of a new instance x by simply computting the decision function.\n",
    "\n",
    "**Equation Linear SVM classifier prediction**\n",
    "\n",
    "$\\hat{y} =  \\left\\{ \\begin{array}{lcc}\n",
    "             0 &   if  & w^{T}x + b < 0 \\\\\n",
    "             \\\\ 1 &  if  & w^{T}x + b \\geq 0\n",
    "             \\end{array}\n",
    "   \\right.$\n",
    "   \n",
    "- w is the feature wieght vector (previously called $\\theta$)\n",
    "- b is the bias term (previously called x_{0} = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the soft margin objective, we need to introduce a *slack variable* for each instance: measures how much the $i^{th}$ instance is allowed to violate the margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
