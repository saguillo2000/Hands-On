{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *Support Vector Machine* (SVM) is a powerful and versatile Machine Learning model, capable of performing linear or nonlinear classification, regression, and even outlier detection. SVMs are particularly well suited for classification of complex small- or medium-sized datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 5.1 good explanaiton\n",
    "\n",
    "You can thjink of an SVM classifier as fitting the widest possible street (represented by the parallel dashed lines) between the class. This is called *larged margin classification*.\n",
    "\n",
    "Notice that adding more training instances \"off the street\" will not affect the decision boundary at all: it is fully determined (or \"supported\") by the instances located on the edge of the street. These instances are called the *support vectors*.\n",
    "\n",
    "SVMs are sensitive to the feature scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Margin Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we strictly impose all instances must be off the street and on the right side, this is called *hard margin classification*.The two main issued are:\n",
    "- It only work if the data is linearly separable.\n",
    "- It is sensitive to outliers\n",
    "\n",
    "The objective is to find a good balance between keeping, the street as large as possible and limiting the *margin violations* (i.e., instances that ends up in the middle of the street or even on the wrong side). This is called *soft margin classification*.\n",
    "\n",
    "If your SVM model is overfitting, you can try regularizing it by reducing C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2,3)]  #petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64) #Iris virginica\n",
    "\n",
    "svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")),\n",
    "])\n",
    "\n",
    "svm_clf.fit(X, y)\n",
    "\n",
    "svm_clf.predict([[5.5, 1.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike Logistic Reegression classifiers, SVM classifiers do not output probabilities for each class.\n",
    "\n",
    "Instead of using the LinearSVC class, we could use the SVC class with a linear kernel (SVC(kernel=\"linear\", C=1)).\n",
    "\n",
    "For better performance ypus hould set the dual hyperparameter to False, unless there are more features tha "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
